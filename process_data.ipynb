{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9766c68e",
      "metadata": {
        "id": "9766c68e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, BooleanType\n",
        "from pyspark.sql.functions import collect_list, udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import first\n",
        "import pyspark\n",
        "from typing import List, Dict, Tuple, Set, Optional, Callable\n",
        "from pyspark.sql import DataFrame, Column, Row"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сначала подготовим датасет с последовательностями конверсий"
      ],
      "metadata": {
        "id": "vssuTKc_BaA9"
      },
      "id": "vssuTKc_BaA9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8GW2YD-JiTAN",
      "metadata": {
        "id": "8GW2YD-JiTAN"
      },
      "outputs": [],
      "source": [
        "def clean_data_actions(data_logs: pyspark.sql.DataFrame, action: str) -> pyspark.sql.DataFrame:\n",
        "    \"\"\"\n",
        "    Обрабатывает датасет, оставляя только данные о нужном действии и добавляя необходимые столбцы.\n",
        "\n",
        "    Args:\n",
        "        data_logs (pyspark.sql.DataFrame): Входной датасет с логами действий пользователей. Должен содержать столбцы:\n",
        "            - action: Действие пользователя (например, 'conversion').\n",
        "            - customer_user_id: Уникальный идентификатор пользователя.\n",
        "            - customer_id: Уникальный идентификатор товара.\n",
        "            - timestamp: Временная метка действия.\n",
        "            - revenue: Доход, полученный от действия.\n",
        "        action (str): Названия действия для фильтрации датасета\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: Очищенный датасет со следующими столбцами:\n",
        "            - user_id: Уникальный идентификатор пользователя.\n",
        "            - item_id: Уникальный идентификатор товара или услуги.\n",
        "            - timestamp: Временная метка конверсии.\n",
        "            - revenue: Доход, полученный от конверсии.\n",
        "    \"\"\"\n",
        "    # Фильтруем данные, оставляя только строки с действием 'conversion'\n",
        "    data_actions = data_logs.filter(data_logs.action == action)\n",
        "\n",
        "    # Оставляем только нужные столбцы: customer_user_id, customer_id, timestamp, revenue\n",
        "    data_actions_filtered = data_actions.select('customer_user_id', 'customer_id', 'timestamp', 'revenue')\n",
        "\n",
        "    # Сортируем данные по customer_user_id и timestamp\n",
        "    data_actions = data_actions_filtered.orderBy(['customer_user_id', 'timestamp'])\n",
        "\n",
        "    # Переименовываем столбцы для удобства\n",
        "    data_actions = data_actions.withColumnRenamed('customer_user_id', 'user_id') \\\n",
        "                      .withColumnRenamed('customer_id', 'item_id')\n",
        "\n",
        "    # Проверяем датасет на пропущенные значения\n",
        "    if check_nan_values(data_actions) > 0:\n",
        "      data_actions = data_actions.dropna()\n",
        "\n",
        "    return data_actions\n",
        "\n",
        "\n",
        "def check_nan_values(data: pyspark.sql.DataFrame) -> int:\n",
        "    \"\"\"\n",
        "    Проверяет пропущенные значения (NULL и NaN) в датасете.\n",
        "\n",
        "    Args:\n",
        "        data (pyspark.sql.DataFrame): Входной датасет.\n",
        "\n",
        "    Returns:\n",
        "        int: число строчек с NULL\n",
        "    \"\"\"\n",
        "    # Подсчёт строк до удаления пропущенных значений\n",
        "    initial_count = data.count()\n",
        "\n",
        "    # Удаляем строки с пропущенными значениями\n",
        "    data_cleaned = data.dropna()\n",
        "\n",
        "    # Подсчёт строк после удаления пропущенных значений\n",
        "    final_count = data_cleaned.count()\n",
        "\n",
        "    # Вычисляем количество удалённых строк\n",
        "    removed_count = initial_count - final_count\n",
        "    print(f\"Найдено строк c пропущенных значений: {removed_count}\")\n",
        "\n",
        "    # Проверяем, в каких колонках есть пропущенные значения\n",
        "    for column in data.columns:\n",
        "        # Получаем тип колонки\n",
        "        column_type = str(data.schema[column].dataType)\n",
        "\n",
        "        # Проверяем только NULL для нечисловых колонок\n",
        "        if column_type not in [\"DoubleType\", \"FloatType\"]:\n",
        "            na_count = data.filter(col(column).isNull()).count()\n",
        "            if na_count > 0:\n",
        "                print(f\"Колонка '{column}' содержит {na_count} пропущенных значений (NULL).\")\n",
        "        else:\n",
        "            # Для числовых колонок проверяем и NULL, и NaN\n",
        "            na_count = data.filter(col(column).isNull() | col(column).isNaN).count()\n",
        "            if na_count > 0:\n",
        "                print(f\"Колонка '{column}' содержит {na_count} пропущенных значений (NULL или NaN).\")\n",
        "\n",
        "    return removed_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def actions_to_sasrec_format(data_actions_cleaned: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
        "    \"\"\"\n",
        "    Преобразует датасет с действиями юзеров в формат, подходящий для модели SASRec.\n",
        "    А именно указывает соотвествующий тип данных в датасете и отбрасывает не нужные столбцы\n",
        "\n",
        "    Args:\n",
        "        data_actions_cleaned (pyspark.sql.DataFrame): Датасет (полученный функцией clean_data_actions от изначального датасета needed_beh_logs)\n",
        "        только с нужными действиями покупателей. Должен содержать столбцы:\n",
        "            - user_id: Уникальный идентификатор пользователя.\n",
        "            - item_id: Уникальный идентификатор товара или услуги.\n",
        "            - timestamp: Временная метка действия.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: Преобразованный датасет со следующими столбцами:\n",
        "            - user_id: Уникальный идентификатор пользователя (тип int).\n",
        "            - item_id: Уникальный идентификатор товара или услуги (тип int).\n",
        "            - datetime: Временная метка действия в формате timestamp.\n",
        "            - weight: Вес действия (всегда 1, тип int).\n",
        "    \"\"\"\n",
        "    # Преобразуем timestamp в строку с датой и временем и добавляем колонку weight\n",
        "    data_actions_processed = data_actions_cleaned.withColumn(\n",
        "        \"datetime\", F.date_format(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")  # Преобразуем timestamp в строку\n",
        "    ).withColumn(\n",
        "        \"weight\", F.lit(1)  # Добавляем колонку weight со значением 1\n",
        "    ).select(\"user_id\", \"item_id\", \"datetime\", \"weight\")  # Оставляем нужные колонки\n",
        "\n",
        "    # Приводим типы данных к нужным форматам\n",
        "    data_actions_processed = data_actions_processed \\\n",
        "        .withColumn(\"user_id\", col(\"user_id\").cast(\"int\")) \\\n",
        "        .withColumn(\"item_id\", col(\"item_id\").cast(\"int\")) \\\n",
        "        .withColumn(\"datetime\", col(\"datetime\").cast(\"timestamp\")) \\\n",
        "        .withColumn(\"weight\", col(\"weight\").cast(\"int\"))\n",
        "\n",
        "    # Проверяем датасет на пропущенные значения\n",
        "    check_nan_values(data_actions_processed)\n",
        "\n",
        "    # Проверяем датасет на пропущенные значения\n",
        "    if check_nan_values(data_actions_processed) > 0:\n",
        "      data_actions_processed = data_actions_processed.dropna()\n",
        "\n",
        "    return data_actions_processed"
      ],
      "metadata": {
        "id": "Ez-vGHlppcIL"
      },
      "id": "Ez-vGHlppcIL",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Теперь подготовим датасет с описанием товаров"
      ],
      "metadata": {
        "id": "kqRNEj2pB6F4"
      },
      "id": "kqRNEj2pB6F4"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql import DataFrame\n",
        "import pyspark\n",
        "\n",
        "def print_metadata(df: pyspark.sql.DataFrame):\n",
        "    \"\"\"Выводит основные метаданные и отдельные значения facets из JSON в колонке metadata_json.\"\"\"\n",
        "    first_row = df.select(\"metadata_json\").first()\n",
        "    if not first_row:\n",
        "        print(\"Датасет пуст.\")\n",
        "        return\n",
        "\n",
        "    metadata = json.loads(first_row[\"metadata_json\"])\n",
        "\n",
        "    # Выводим основные метаданные\n",
        "    print(\"Основные метаданные:\")\n",
        "    for key, value in metadata.items():\n",
        "        if key != \"facets\":\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    # Обрабатываем ключ facets отдельно\n",
        "    if \"facets\" in metadata:\n",
        "        print(\"\\nFacets:\")\n",
        "        facets = json.loads(metadata[\"facets\"]) if isinstance(metadata[\"facets\"], str) else metadata[\"facets\"]\n",
        "        for key, value in facets.items():\n",
        "            print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "DuB9feY1MP9P"
      },
      "id": "DuB9feY1MP9P",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "import pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "from typing import List, Optional\n",
        "\n",
        "def parse_metadata_json(data_items: pyspark.sql.DataFrame, metadata: List[str], facets: Optional[List[str]] = None) -> pyspark.sql.DataFrame:\n",
        "    \"\"\"Извлекает данные из metadata_json и добавляет их как отдельные колонки.\"\"\"\n",
        "    metadata_schema_fields = [\n",
        "        StructField(field, ArrayType(StringType()), True) if field == \"group_ids\" else StructField(field, StringType(), True)\n",
        "        for field in metadata\n",
        "    ]\n",
        "\n",
        "    if facets:\n",
        "        metadata_schema_fields.append(\n",
        "            StructField(\"facets\", StructType([StructField(facet, ArrayType(StringType()), True) for facet in facets]), True)\n",
        "        )\n",
        "\n",
        "    metadata_schema = StructType(metadata_schema_fields)\n",
        "\n",
        "    data_items_parsed = data_items.withColumn(\n",
        "        \"metadata_parsed\", F.from_json(F.col(\"metadata_json\"), metadata_schema)\n",
        "    )\n",
        "\n",
        "    selected_columns = [\"id\", \"customer_id\", \"name\", \"name_lower\"] + [f\"metadata_parsed.{field}\" for field in metadata]\n",
        "    if facets:\n",
        "        selected_columns.append(\"metadata_parsed.facets\")\n",
        "\n",
        "    return data_items_parsed.select(*selected_columns)\n",
        "\n",
        "def find_intersection(data_items: pyspark.sql.DataFrame, group_col: str, value_col: str) -> pyspark.sql.DataFrame:\n",
        "    \"\"\"Находит пересечение значений в колонке value_col для каждой группы, определенной group_col.\n",
        "\n",
        "    Args:\n",
        "        data_items (DataFrame): Входной датасет.\n",
        "        group_col (str): Название колонки, по которой будет группировка (например, customer_id).\n",
        "        value_col (str): Название колонки, для которой ищется пересечение значений. (например, group_ids)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Датасет с group_col и пересечением значений из value_col.\n",
        "    \"\"\"\n",
        "    exploded_df = data_items.select(group_col, F.explode(value_col).alias(\"value\"))\n",
        "\n",
        "    grouped_df = exploded_df.groupBy(group_col, \"value\").agg(F.count(\"*\").alias(\"count\"))\n",
        "    total_lists_per_group = data_items.groupBy(group_col).agg(F.count(\"*\").alias(\"total_lists\"))\n",
        "\n",
        "    joined_df = grouped_df.join(total_lists_per_group, on=group_col, how=\"left\")\n",
        "\n",
        "    return joined_df.filter(F.col(\"count\") == F.col(\"total_lists\")) \\\n",
        "        .groupBy(group_col) \\\n",
        "        .agg(F.collect_list(\"value\").alias(f\"{value_col}_intersect\"))\n",
        "\n",
        "def aggregate_data_items(data_items: pyspark.sql.DataFrame, metadata: List[str], facets: Optional[List[str]] = None) -> pyspark.sql.DataFrame:\n",
        "    \"\"\"Агрегирует данные по customer_id, беря первые значения одинаковых колонок.\n",
        "       В тех колонках, где внутри одного customer_id значения не меняются, берем первые из этих значений для опредления признака данного customer_id\n",
        "    \"\"\"\n",
        "    same_columns = ['name', 'name_lower'] + metadata\n",
        "    if facets:\n",
        "        same_columns.append(\"facets\")\n",
        "\n",
        "    return data_items.groupBy(\"customer_id\").agg(\n",
        "        *[F.first(col_name).alias(col_name) for col_name in same_columns if col_name != \"customer_id\"]\n",
        "    )\n",
        "\n",
        "def clean_data_items(data_items: pyspark.sql.DataFrame, metadata: List[str], facets: Optional[List[str]] = None) -> pyspark.sql.DataFrame:\n",
        "    \"\"\"Убирает лишние колонки, разворачивает нужные метаданные и facets в новые колонки, а также агрегирует признаки внутри одних customer_id\"\"\"\n",
        "    data_items_filtered = data_items.select(\"id\", \"customer_id\", \"name\", \"name_lower\", \"metadata_json\")\n",
        "    data_items_parsed = parse_metadata_json(data_items_filtered, metadata, facets)\n",
        "\n",
        "    aggregated_same = aggregate_data_items(data_items_parsed, metadata, facets)\n",
        "\n",
        "    # Находим внутри каждого customer_id перечение group_ids\n",
        "    if \"group_ids\" in metadata:\n",
        "        intersected_df = find_intersection(data_items_parsed, \"customer_id\", \"group_ids\")\n",
        "        final_df = aggregated_same.join(intersected_df.select(\"customer_id\", \"group_ids_intersect\"), on=\"customer_id\", how=\"left\")\n",
        "        final_df = final_df.drop(\"group_ids\")\n",
        "    else:\n",
        "        final_df = aggregated_same\n",
        "\n",
        "    # Извлекаем отдельные атрибуты из facets, если facets передан, и вставляем их в датасет в качестве новых колонок\n",
        "    if facets:\n",
        "        for facet in facets:\n",
        "            final_df = final_df.withColumn(facet.replace(\" \", \"_\"), F.col(f\"facets.`{facet}`\"))\n",
        "        final_df = final_df.drop(\"facets\")\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "aE1wQL09Ct7G"
      },
      "execution_count": 8,
      "outputs": [],
      "id": "aE1wQL09Ct7G"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "import pyspark\n",
        "from typing import List\n",
        "\n",
        "def items_to_sasrec_format(\n",
        "    data_items_cleaned: pyspark.sql.DataFrame, # результат функции clean_data_items от изначального датасета data_set_items\n",
        "    features: List[str],\n",
        "    features_names_out: List[str]\n",
        ") -> pyspark.sql.DataFrame:\n",
        "    \"\"\"Преобразует данные о товарах в формат SASRec.\n",
        "\n",
        "    Args:\n",
        "        data_items_cleaned (DataFrame): Исходный DataFrame с товарами.\n",
        "        features (List[str]): Список названий колонок с фичами.\n",
        "        features_names_out (List[str]): Список имен фичей для выхода.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Преобразованный DataFrame с фичами в формате SASRec.\n",
        "    \"\"\"\n",
        "    df = data_items_cleaned.select(*(['customer_id'] + features))\n",
        "    df = df.withColumn('customer_id', F.col('customer_id').cast('int'))\n",
        "    dfs_feature = []\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        feature = df.select(\n",
        "            \"customer_id\",\n",
        "            F.explode(features[i]).alias(\"value\")\n",
        "        ).withColumn(\"feature\", F.lit(features_names_out[i])).withColumnRenamed(\"customer_id\", \"id\")\n",
        "        dfs_feature.append(feature)\n",
        "\n",
        "    data_items_processed = dfs_feature[0]\n",
        "    for i in range(1, len(features)):\n",
        "        data_items_processed = data_items_processed.union(dfs_feature[i])\n",
        "\n",
        "    return data_items_processed\n"
      ],
      "metadata": {
        "id": "5TvNgn-11AnL"
      },
      "execution_count": 9,
      "outputs": [],
      "id": "5TvNgn-11AnL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Прочие функции\n"
      ],
      "metadata": {
        "id": "wW4RHH0ZOmth"
      },
      "id": "wW4RHH0ZOmth"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset_parquet(dataset: pyspark.sql.DataFrame, folder_path: str, file_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Сохраняет датасет в формате Parquet по указанному пути.\n",
        "\n",
        "    Args:\n",
        "        dataset (pyspark.sql.DataFrame): Датасет, который нужно сохранить.\n",
        "        folder_path (str): Путь к папке, куда будет сохранён файл.\n",
        "        file_name (str): Имя файла для сохранения (без расширения).\n",
        "\n",
        "    Returns:\n",
        "        None: Функция не возвращает значение, но сохраняет датасет на диск.\n",
        "    \"\"\"\n",
        "    # Проверяем, существует ли папка, если нет - создаем\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Папка {folder_path} успешно создана!\")\n",
        "    else:\n",
        "        print(f\"Папка {folder_path} уже существует.\")\n",
        "\n",
        "    # Формируем полный путь к файлу\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    # Сохранение в формате Parquet\n",
        "    dataset.write.parquet(file_path, mode=\"overwrite\")\n",
        "    print(f\"Датасет сохранён по пути {file_path}.\")"
      ],
      "metadata": {
        "id": "wNoOIFENOqA6"
      },
      "id": "wNoOIFENOqA6",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f6kCt81AOqv4"
      },
      "id": "f6kCt81AOqv4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}