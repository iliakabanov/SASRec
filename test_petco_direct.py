# -*- coding: utf-8 -*-
"""test_petco_direct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmQejt7mVZVIbzYNSm9KriHO8yvnRy04
"""

# Import libraries
from pyspark.sql import SparkSession, Window, Row
from pyspark.sql import functions as F
import pandas as pd
from typing import List, Dict, Tuple, Set, Optional, Callable
from datetime import datetime, timedelta
import pyspark
import pytest
from chispa.dataframe_comparer import assert_df_equality
import random

import sys
sys.path.append("/content/drive/MyDrive/Colab Notebooks/diploma/scripts")
import process_data

# PARAMETERS
WORKING_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/diploma/'
path_raw_actions = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/raw/actions'
path_raw_items = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/raw/items'
path_expected_actions = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/sasrec_format_expected/actions'
path_expected_items = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/sasrec_format_expected/items'
action = 'conversion'
customer_ids = [5017277, 5023023, 5024527]
metadata = ['url', 'image_url', 'itemname', 'group_ids']
facets = ['How to get it', 'Primary Brand', 'Pet Type', 'Primary Pet Type']
features = ['group_ids_intersect', 'How_to_get_it', 'Primary_Brand', 'Primary_Pet_Type']
features_final_names = ['Category', 'Delivery', 'Brand', 'Pet']

def create_raw_actions_dataset(customer_ids: List[int]):
  '''
  Params:
      customer_ids (List[int]): Список id товаров для игрушечного примера

  Returns:
      pyspark.sql.DataFrame: Игрушечный пример сырого датасета с действиями трех юзеров
  '''

  data_logs = spark.read.parquet(WORKING_DIRECTORY+'data/needed_beh_logs')

  # Генерация данных
  data = []
  conversions = []

  for i in range(3):  # 3 уникальных юзера
      user_id = random.randint(10**11, 10**12 - 1)  # 12-значный ID юзера

      num_purchases = random.randint(1, 3)  # от 1 до 3 покупок
      num_conversions = random.randint(2, 4)  # от 2 до 4 конверсий

      for _ in range(num_purchases):
          # Генерируем случайное время для покупки с использованием datetime
          timestamp = datetime.now() + timedelta(seconds=random.randint(0, 100000))
          timestamp_str = timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')

          # Добавляем строку с покупкой
          data.append((user_id, random.choice(customer_ids), "purchase", timestamp_str, round(random.uniform(10, 100), 2)))

      for _ in range(num_conversions):
          # Генерируем случайное время и товар для конверсии
          timestamp = datetime.now() + timedelta(seconds=random.randint(0, 100000))
          timestamp_str = timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')
          item_id = random.choice(customer_ids)

          # Добавляем и запоминаем строку с конверсией
          conversions.append([user_id, item_id, 'conversion', timestamp_str])
          data.append((user_id, item_id, "conversion", timestamp_str, None))

  # Создание DataFrame
  columns = ["customer_user_id", "customer_id", "action", "timestamp", "revenue"]
  df_logs = spark.createDataFrame(data, columns)

  # Преобразуем столбцы в тип string и timestamp
  data_logs_example = df_logs.withColumn("customer_user_id", F.col("customer_user_id").cast("string")) \
                  .withColumn("customer_id", F.col("customer_id").cast("string")) \
                  .withColumn("timestamp", F.col("timestamp").cast("timestamp"))

  return data_logs_example, conversions

def create_raw_items_dataset(customer_ids: List[int]):
  '''
  Params:
      customer_ids (List[int]): Список id товаров для игрушечного примера

  Returns:
      pyspark.sql.DataFrame: Игрушечный пример сырого датасета с признаками трех товаров
  '''
  data_items = spark.read.parquet(WORKING_DIRECTORY+'data/data_set_items')

  # Фильтруем датасет по выбранным customer_id
  filtered_items = data_items.filter(F.col('customer_id').isin(customer_ids))

  # Оставляем только по 2 строки для каждого customer_id
  filtered_items = (filtered_items
      .withColumn("row_num", F.row_number().over(Window.partitionBy("customer_id").orderBy(F.rand(seed=42))))
      .filter(F.col("row_num") <= 2)
      .drop("row_num"))

  return filtered_items

def create_expected_actions_dataset(conversions: List[List]):
  '''
  Params:
      conversions (List[List]): Строки сырого игрушечного датасета товаров

  Returns:
      pyspark.sql.DataFrame: Ожидаемый вид датасета с действиями юзеров в формате SASRec
  '''
  conversions = [[raw[0], raw[1], pd.to_datetime(raw[3].split('.')[0]), 1] for raw in conversions]
  data_actions_expected = pd.DataFrame(columns=['user_id', 'item_id', 'datetime', 'weight'], data=conversions)
  data_actions_expected = spark.createDataFrame(data_actions_expected)

  data_actions_expected = data_actions_expected \
      .withColumn("user_id", F.col("user_id").cast("bigint")) \
      .withColumn("item_id", F.col("item_id").cast("int")) \
      .withColumn("weight", F.col("weight").cast("int")).orderBy(['user_id', 'datetime'])

  return data_actions_expected

def create_expected_items_dataset(customer_ids: List[int]):
  '''
  Params:
      customer_ids (List[int]): Список id товаров для игрушечного примера

  Returns:
      pyspark.sql.DataFrame: Ожидаемый вид датасета с признаками трех товаров в формате SASRec
  '''
  group_ids_1 = ['small-animal-repeat-delivery-products', 'repeat-delivery-eligible-products', 'rat-food', 'small-animal-food', 'buy-online-pick-up-in-store-small-animal-products', 'same-day-delivery-small-pet-products']
  deliveries_1 = ['Same Day Delivery', 'Free Pickup Today', 'One Time Delivery', 'Repeat Delivery']
  brands_1 = ['Kaytee']
  pets_1 = ['Small Animal']
  item_1 = [*[[customer_ids[0], category, 'Category'] for category in group_ids_1], \
          *[[customer_ids[0], delivery, 'Delivery'] for delivery in deliveries_1], \
          *[[customer_ids[0], brand, 'Brand'] for brand in brands_1], \
          *[[customer_ids[0], pet, 'Pet'] for pet in pets_1]
  ]

  group_ids_2 = ['cat-doors-and-flaps', 'dog-patio-mount-doors']
  deliveries_2 = ["One Time Delivery"]
  brands_2 = ['Perfect Pet']
  pets_2 = ['Cat', 'Dog']
  item_2 = [*[[customer_ids[1], category, 'Category'] for category in group_ids_2], \
          *[[customer_ids[1], delivery, 'Delivery'] for delivery in deliveries_2], \
          *[[customer_ids[1], brand, 'Brand'] for brand in brands_2], \
          *[[customer_ids[1], pet, 'Pet'] for pet in pets_2]
  ]

  group_ids_3 = ["fall-shop-food","canned-cat-food","premium-cat-food","repeat-delivery-eligible-products"]
  deliveries_3 = ["One Time Delivery"]
  brands_3 = ["Tiki Cat"]
  pets_3 = ['Cat']
  item_3 = [*[[customer_ids[2], category, 'Category'] for category in group_ids_3], \
          *[[customer_ids[2], delivery, 'Delivery'] for delivery in deliveries_3], \
          *[[customer_ids[2], brand, 'Brand'] for brand in brands_3], \
          *[[customer_ids[2], pet, 'Pet'] for pet in pets_3]
  ]

  items = item_1 + item_2 + item_3
  # Преобразование данных в список Row
  rows = [Row(id=item[0], value=item[1], feature=item[2]) for item in items]

  # Создание DataFrame из списка Row
  df = spark.createDataFrame(rows)
  df = df.withColumn("id", F.col("id").cast("int")).select('id', 'value', 'feature')

  return df.orderBy('id', 'feature', 'value')

"""# Тест для датасета с признаками товаров в формате SASRec"""

# Создаем SparkSession
spark = SparkSession.builder \
    .appName("PetCo_direct_tests") \
    .getOrCreate()

# Создаем игрушечный пример сырого датасета с признаками трех товаров
data_items_raw = create_raw_items_dataset(customer_ids)
# Руками строим ожидаемый вид датасета с признаками товаров в формате SASRec
data_items_expected = create_expected_items_dataset(customer_ids)

@pytest.mark.parametrize("data_items_raw, data_items_expected, metadata, facets, features, features_final_names", \
 [(data_items_raw, data_items_expected, metadata, facets, features, features_final_names)])
def test_items_sasrec(data_items_raw : pyspark.sql.DataFrame,
                      data_items_expected: pyspark.sql.DataFrame,
                      metadata: List[str],
                      facets: List[str],
                      features: List[str],
                      features_final_names: List[str],):

  data_items_cleaned = process_data.clean_data_items(data_items_raw, metadata = metadata, facets = facets)
  data_items_processed = process_data.items_to_sasrec_format(data_items_cleaned, features, features_final_names)

  assert_df_equality(data_items_processed, data_items_expected, ignore_nullable=True)

"""# Тест для датасета с признакми товаров в формате SASRec"""

# Создаем игрушечный пример сырого датасета с действиями юзеров
data_actions_raw, conversions = create_raw_actions_dataset(customer_ids)
# Руками строим ожидаемый вид датасета с действиями юзеров в формате SASRec
data_actions_expected = create_expected_actions_dataset(conversions)

@pytest.mark.parametrize("path_raw_actions, path_expected_actions, action", [(data_actions_raw, data_actions_expected, action)])
def test_actions_sasrec(path_raw_actions : str, path_expected_actions: str, action: str):
  data_actions_cleaned = process_data.clean_data_actions(data_actions_raw, action)
  data_actions_processed = process_data.actions_to_sasrec_format(data_actions_cleaned)

  assert_df_equality(data_actions_processed, data_actions_expected, ignore_nullable=True)

