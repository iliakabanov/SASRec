# -*- coding: utf-8 -*-
"""test_petco_direct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmQejt7mVZVIbzYNSm9KriHO8yvnRy04
"""

# Import libraries
import pyspark
from pyspark.sql import SparkSession, Window, Row
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, DoubleType, IntegerType
import json
import pandas as pd
from typing import List, Dict, Tuple, Set, Optional, Callable
from datetime import datetime, timedelta
from chispa.dataframe_comparer import assert_df_equality
import random
import pytest
import process_data

# PARAMETERS
action = 'conversion'
customer_ids = [100001, 100002, 100003]
metadata = ['url', 'image_url', 'itemname', 'group_ids']
facets = ['How to get it', 'Primary Brand', 'Pet Type', 'Primary Pet Type']
features = ['group_ids_intersect', 'How_to_get_it', 'Primary_Brand', 'Primary_Pet_Type']
features_final_names = ['Category', 'Delivery', 'Brand', 'Pet']

def create_raw_actions_dataset(customer_ids: List[int]):
  '''
  Params:
      customer_ids (List[int]): Список id товаров для игрушечного примера

  Returns:
      pyspark.sql.DataFrame: Игрушечный пример сырого датасета с действиями трех юзеров
  '''
  # Определяем схему датасета
  schema = StructType([
      StructField("customer_user_id", StringType(), False),
      StructField("timestamp", TimestampType(), False),
      StructField("action", StringType(), False),
      StructField("customer_id", StringType(), True),
      StructField("revenue", DoubleType(), False)
  ])

  # Создаем тестовые данные с обновленным metadata_json
  raw_data = [
      ('11111111111', datetime(2024, 9, 4, 20, 45, 1, 1), 'conversion', customer_ids[0], 10.99),
      ('11111111111', datetime(2024, 9, 4, 20, 50, 1, 1), 'conversion', customer_ids[1], 22.99),
      ('11111111111', datetime(2024, 9, 4, 20, 55, 1, 1), 'purchase', None, 10.99),
      ('11111111111', datetime(2024, 9, 4, 21, 00, 1, 1), 'conversion', customer_ids[0], 20.99),
      ('11111111112', datetime(2024, 10, 4, 20, 45, 1, 1), 'conversion', customer_ids[2], 30.99),
      ('11111111112', datetime(2024, 10, 4, 20, 50, 1, 1), 'purchase', None, 30.99),
      ('11111111112', datetime(2024, 10, 4, 20, 55, 1, 1), 'conversion', customer_ids[1], 22.99),
      ('11111111113', datetime(2024, 11, 4, 21, 10, 1, 1), 'conversion', customer_ids[0], 10.99),
      ('11111111113', datetime(2024, 11, 4, 20, 45, 1, 1), 'conversion', customer_ids[2], 25.99),
      ('11111111113', datetime(2024, 11, 4, 20, 50, 1, 1), 'purhase', None, 25.99),
      ('11111111113', datetime(2024, 11, 4, 20, 55, 1, 1), 'purchase', None, 10.99),
      ('11111111113', datetime(2024, 11, 4, 21, 00, 1, 1), 'conversion', customer_ids[1], 22.99)
  ]

  # Создаем DataFrame
  df = spark.createDataFrame(raw_data, schema=schema)

  return df

def create_raw_items_dataset(customer_ids: List[int]):
  '''
  Params:
      customer_ids (List[int]): Список id товаров для игрушечного примера

  Returns:
      pyspark.sql.DataFrame: Игрушечный пример сырого датасета с признаками трех товаров
  '''
  # Определяем схему датасета
  schema = StructType([
      StructField("id", LongType(), False),
      StructField("name", StringType(), False),
      StructField("customer_id", StringType(), False),
      StructField("name_lower", StringType(), False),
      StructField("metadata_json", StringType(), False)
  ])

  # Создаем тестовые данные с обновленным metadata_json
  raw_data = [
      (100000000001, "Purina Chicken wings 1 kg", str(customer_ids[0]), "dog food", json.dumps({
          "group_ids": ["dog-food", "repeat-dog-products"],
          "facets": {
              "Primary Brand": ["Purina"],
              "Primary Pet Type": ["Dog,Snake"],
              "How to get it": ["Same Day Delivery", "Free Pickup Today", "One Time Delivery", "Repeat Delivery"]
          }
      })),
      (100000000002, "Purina Chicken wings 2 kg", str(customer_ids[0]), "dog food", json.dumps({
          "group_ids": ["dog-food", "repeat-dog-products"],
          "facets": {
              "Primary Brand": ["Purina"],
              "Primary Pet Type": ["Dog,Snake"],
              "How to get it": ["Same Day Delivery", "Free Pickup Today", "One Time Delivery", "Repeat Delivery"]
          }
      })),
      (100000000003, "Wiskas Cat Litter Big", str(customer_ids[1]), "cat litter", json.dumps({
          "group_ids": ["cat-litter", "repeat-cat-products", 'big-cats'],
          "facets": {
              "Primary Brand": ["Whiskas"],
              "Primary Pet Type": ["Cat"],
              "How to get it": ["Same Day Delivery", "Free Pickup Today"]
          }
      })),
      (100000000004, "Wiskas Cat Litter", str(customer_ids[1]), "cat litter", json.dumps({
          "group_ids": ["cat-litter", "repeat-cat-products"],
          "facets": {
              "Primary Brand": ["Whiskas"],
              "Primary Pet Type": ["Cat"],
              "How to get it": ["Same Day Delivery", "Free Pickup Today"]
          }
      })),
      (100000000005, "Fish Tank AquaWorld 20l", str(customer_ids[2]), "fish tank", json.dumps({
          "group_ids": ["fish-tanks", "aquarium-supplies", 'stupid-staff'],
          "facets": {
              "Primary Brand": ["AquaWorld"],
              "Primary Pet Type": ["Fish"],
          }
      })),
      (100000000006, "Fish Tank AquaWorld 10l", str(customer_ids[2]), "fish tank", json.dumps({
          "group_ids": ["fish-tanks", "aquarium-supplies"],
          "facets": {
              "Primary Brand": ["AquaWorld"],
              "Primary Pet Type": ["Fish"]
          }
      }))
  ]

  # Создаем DataFrame
  df = spark.createDataFrame(raw_data, schema=schema)

  return df

def create_expected_actions_dataset(customer_ids: List[int]):
  '''
  Params:
      customer_ids (List[int]): Список id товаров для игрушечного примера

  Returns:
      pyspark.sql.DataFrame: Ожидаемый вид датасета с действиями юзеров в формате SASRec
  '''
   # Определяем схему датасета
  schema = StructType([
      StructField("user_id", LongType(), False),
      StructField("item_id", IntegerType(), False),
      StructField("datetime", TimestampType(), False),
      StructField("weight", IntegerType(), True)
  ])

  # Создаем тестовые данные с обновленным metadata_json
  raw_data = [
      (11111111111, customer_ids[0], datetime(2024, 9, 4, 20, 45, 1), 1),
      (11111111111, customer_ids[1], datetime(2024, 9, 4, 20, 50, 1), 1),
      (11111111111, customer_ids[0], datetime(2024, 9, 4, 21, 00, 1), 1),
      (11111111112, customer_ids[2], datetime(2024, 10, 4, 20, 45, 1), 1),
      (11111111112, customer_ids[1], datetime(2024, 10, 4, 20, 55, 1), 1),
      (11111111113, customer_ids[0], datetime(2024, 11, 4, 21, 10, 1), 1),
      (11111111113, customer_ids[2], datetime(2024, 11, 4, 20, 45, 1), 1),
      (11111111113, customer_ids[1], datetime(2024, 11, 4, 21, 00, 1), 1)
  ]

  # Создаем DataFrame
  df = spark.createDataFrame(raw_data, schema=schema).orderBy(['user_id', 'datetime'])

  return df

def create_expected_items_dataset(customer_ids: List[int]):
  '''
  Params:
      customer_ids (List[int]): Список id товаров для игрушечного примера

  Returns:
      pyspark.sql.DataFrame: Ожидаемый вид датасета с признаками трех товаров в формате SASRec
  '''
  group_ids_1 = ["dog-food", "repeat-dog-products"]
  deliveries_1 = ["Same Day Delivery", "Free Pickup Today", "One Time Delivery", "Repeat Delivery"]
  brands_1 = ["Purina"]
  pets_1 = ['Dog', 'Snake']
  item_1 = [*[[customer_ids[0], category, 'Category'] for category in group_ids_1], \
          *[[customer_ids[0], delivery, 'Delivery'] for delivery in deliveries_1], \
          *[[customer_ids[0], brand, 'Brand'] for brand in brands_1], \
          *[[customer_ids[0], pet, 'Pet'] for pet in pets_1]
  ]

  group_ids_2 = ["cat-litter", "repeat-cat-products"]
  deliveries_2 = ["Same Day Delivery", "Free Pickup Today"]
  brands_2 = ['Whiskas']
  pets_2 = ['Cat']
  item_2 = [*[[customer_ids[1], category, 'Category'] for category in group_ids_2], \
          *[[customer_ids[1], delivery, 'Delivery'] for delivery in deliveries_2], \
          *[[customer_ids[1], brand, 'Brand'] for brand in brands_2], \
          *[[customer_ids[1], pet, 'Pet'] for pet in pets_2]
  ]

  group_ids_3 = ["fish-tanks", "aquarium-supplies"]
  brands_3 = ["AquaWorld"]
  pets_3 = ['Fish']
  item_3 = [*[[customer_ids[2], category, 'Category'] for category in group_ids_3], \
          *[[customer_ids[2], brand, 'Brand'] for brand in brands_3], \
          *[[customer_ids[2], pet, 'Pet'] for pet in pets_3]
  ]

  items = item_1 + item_2 + item_3
  # Преобразование данных в список Row
  rows = [Row(id=item[0], value=item[1], feature=item[2]) for item in items]

  # Создание DataFrame из списка Row
  df = spark.createDataFrame(rows)
  df = df.withColumn("id", F.col("id").cast("int")).select('id', 'value', 'feature')

  return df.orderBy('id', 'feature', 'value')


"""# Тест для датасета с признаками товаров в формате SASRec"""

# creating SparkSession
spark = SparkSession.builder.appName('Petco_Direct_Tests').getOrCreate()

# Создаем игрушечный пример сырого датасета с признаками трех товаров
data_items_raw = create_raw_items_dataset(customer_ids)
# Руками строим ожидаемый вид датасета с признаками товаров в формате SASRec
data_items_expected = create_expected_items_dataset(customer_ids)

@pytest.mark.parametrize("data_items_raw, data_items_expected, metadata, facets, features, features_final_names", \
 [(data_items_raw, data_items_expected, metadata, facets, features, features_final_names)])
def test_items_sasrec(data_items_raw : pyspark.sql.DataFrame,
                      data_items_expected: pyspark.sql.DataFrame,
                      metadata: List[str],
                      facets: List[str],
                      features: List[str],
                      features_final_names: List[str],):

  data_items_cleaned = process_data.clean_data_items(data_items_raw, metadata = metadata, facets = facets)
  data_items_processed = process_data.items_to_sasrec_format(data_items_cleaned, features, features_final_names)

  assert_df_equality(data_items_processed, data_items_expected, ignore_nullable=True)


"""# Тест для датасета с признакми товаров в формате SASRec"""

# Создаем игрушечный пример сырого датасета с действиями юзеров
data_actions_raw = create_raw_actions_dataset(customer_ids)
# Руками строим ожидаемый вид датасета с действиями юзеров в формате SASRec
data_actions_expected = create_expected_actions_dataset(customer_ids)

@pytest.mark.parametrize("data_actions_raw, data_actions_expected, action", [(data_actions_raw, data_actions_expected, action)])
def test_actions_sasrec(data_actions_raw : pyspark.sql.DataFrame,  data_actions_expected: pyspark.sql.DataFrame, action: str):
  data_actions_cleaned = process_data.clean_data_actions(data_actions_raw, action)
  data_actions_processed = process_data.actions_to_sasrec_format(data_actions_cleaned)

  assert_df_equality(data_actions_processed, data_actions_expected, ignore_nullable=True)


