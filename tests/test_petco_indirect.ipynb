{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6z47sb7Y1T-"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import pyspark\n",
        "from pyspark.sql.types import IntegerType, TimestampType, DoubleType\n",
        "from pyspark.sql.functions import col, collect_set, to_timestamp, sum as spark_sum\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import random\n",
        "from typing import List, Dict, Optional\n",
        "import pytest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/diploma/scripts\")\n",
        "import process_data"
      ],
      "metadata": {
        "id": "wYGUKJ9-VhIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4BGeehhYtwi"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS\n",
        "HEAD_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/diploma/'\n",
        "path_raw_actions = HEAD_DIRECTORY+'data/needed_beh_logs'\n",
        "path_raw_items = HEAD_DIRECTORY+'data/data_set_items'\n",
        "path_processed_actions = HEAD_DIRECTORY+'data/sasrec_format/actions'\n",
        "path_processed_items = HEAD_DIRECTORY+'data/sasrec_format/items'\n",
        "action = 'conversion'\n",
        "metadata_original_names = ['group_ids']\n",
        "facets_original_names = ['How to get it', 'Primary Brand', 'Primary Pet Type']\n",
        "metadata_final_names = ['Category']\n",
        "facets_final_names = ['Delivery', 'Brand', 'Pet']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тесты для датасета с действиями юзеров в формате SASRec"
      ],
      "metadata": {
        "id": "_OCBk0fbjOnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PetCo_indirect_tests\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "79xKXQzaaWvR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "12954938-d6db-4bd7-cafd-777a59d7abc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SparkSession' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04726b319615>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Создаем SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PetCo_indirect_tests\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Читаем файлы\n",
        "data_raw_actions = spark.read.parquet(path_raw_actions)\n",
        "data_processed_actions = spark.read.parquet(path_processed_actions)"
      ],
      "metadata": {
        "id": "EI-2lLaOLGCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.mark.parametrize(\"df\", [(data_processed_actions)])\n",
        "def test_actions_structure(df: pyspark.sql.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет структуру датасета: названия колонок и типы данных.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): Предобработанный датасет с действиями юзеров в формате SASRec.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "\n",
        "    expected_columns = {\"user_id\", \"item_id\", \"datetime\", \"weight\"}\n",
        "    actual_columns = set(df.columns)\n",
        "    assert actual_columns == expected_columns, f\"Ошибка: Ожидались колонки {expected_columns}, но получены {actual_columns}\"\n",
        "\n",
        "    # Получаем схему DataFrame\n",
        "    schema = dict(df.dtypes)\n",
        "\n",
        "    # Проверка типов данных\n",
        "    assert schema[\"user_id\"] == \"bigint\", \"Ошибка: 'user_id' должен быть большим целым числом\"\n",
        "    assert schema[\"item_id\"] == \"int\", \"Ошибка: 'item_id' должен быть целым числом\"\n",
        "    assert schema[\"datetime\"] == \"timestamp\", \"Ошибка: 'datetime' должен быть datetime\"\n",
        "    assert schema[\"weight\"] == \"int\", \"Ошибка: 'weight' должен быть числом\"\n",
        "\n",
        "    print(\"✅ В датасете правильные колонки и у них правильные типы данных\")\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"df\", [(data_processed_actions)])\n",
        "def test_missing_values_actions(df: pyspark.sql.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет наличие пропущенных значений в датасете.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): Предобработанный датасет с действиями юзеров в формате SASRec.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    # spark, process_data = start_script()\n",
        "\n",
        "    missing_values = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "\n",
        "    missing_dict = missing_values.collect()[0].asDict()\n",
        "    total_missing = sum(missing_dict.values())\n",
        "\n",
        "    assert total_missing == 0, f\"Ошибка: Найдены пропущенные значения\\n{missing_dict}\"\n",
        "\n",
        "    print(\"✅ В датасете нет пропущенных значений\")\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"df_final, df_original, action\", [(data_processed_actions, data_raw_actions, action)])\n",
        "def test_number_users(df_final: pyspark.sql.DataFrame, df_original: pyspark.sql.DataFrame, action: str) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет, совпадает ли количество уникальных пользователей в обработанном и исходном датасете.\n",
        "\n",
        "    Args:\n",
        "        df_final (pyspark.sql.DataFrame): Предобработанный датасет с действиями юзеров в формате SASRec.\n",
        "        df_original (pyspark.sql.DataFrame): Исходный датасет с действиями юзеров (needed_beh_logs).\n",
        "        action (str): Название действия (например, 'click', 'purchase').\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "\n",
        "    n_users_original = (\n",
        "        df_original.filter(col(\"action\") == action)\n",
        "        .select(\"customer_user_id\")\n",
        "        .distinct()\n",
        "        .count()\n",
        "    )\n",
        "\n",
        "    n_users_final = df_final.select(\"user_id\").distinct().count()\n",
        "\n",
        "    assert n_users_final == n_users_original, \"Ошибка: неправильное число юзеров\"\n",
        "\n",
        "    print(\"✅ В датасете правильное число юзеров, которые сделали действие\")\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"df_final, df_original, action\", [(data_processed_actions, data_raw_actions, action)])\n",
        "def test_users_actions(df_final: pyspark.sql.DataFrame, df_original: pyspark.sql.DataFrame, action: str, n_users: int = 3) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет, что для случайных пользователей корректно указаны товары и время действий.\n",
        "\n",
        "    Args:\n",
        "        df_final (pyspark.sql.DataFrame): Финальный датасет с действиями юзеров в формате SASRec.\n",
        "        df_original (pyspark.sql.DataFrame): Исходный датасет с действиями юзеров (needed_beh_logs)\n",
        "        action (str): Название действия юзеров.\n",
        "        n_users (int, optional): Количество пользователей для проверки. По умолчанию 10.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    users = df_final.select(\"user_id\").distinct().rdd.flatMap(lambda x: x).takeSample(False, n_users)\n",
        "\n",
        "    for user in tqdm(users):\n",
        "        check_user_actions(user, df_final, df_original, action)\n",
        "\n",
        "    print(\"\\n✅ Для всех юзеров правильно указаны товары и время действий\")\n",
        "\n",
        "\n",
        "def check_user_actions(user_id: int, df_final: pyspark.sql.DataFrame, df_original: pyspark.sql.DataFrame, action: str) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет, совпадают ли товары и время действий пользователя в обработанном и исходном датасете.\n",
        "\n",
        "    Args:\n",
        "        user_id (int): Идентификатор пользователя.\n",
        "        df_final (pyspark.sql.DataFrame): Предобработанный датасет с действиями юзеров в формате SASRec.\n",
        "        df_original (pyspark.sql.DataFrame): Исходный датасет с действиями юзеров (needed_beh_logs)\n",
        "        action (str): Название действия.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    assert get_user_items_from_final(user_id, df_final) == get_user_items_from_original(user_id, df_original, action), \\\n",
        "        f\"Ошибка: неправильно указаны набор товаров и время действий для юзера {user_id}\"\n",
        "\n",
        "\n",
        "def get_user_items_from_final(user_id: int, df: pyspark.sql.DataFrame) -> set[tuple[int, datetime]]:\n",
        "    \"\"\"\n",
        "    Получает множество пар (item_id, datetime) для пользователя из обработанного датасета.\n",
        "\n",
        "    Args:\n",
        "        user_id (int): Идентификатор пользователя.\n",
        "        df (pyspark.sql.DataFrame): Финальный датасет с действиями юзеров\n",
        "\n",
        "    Returns:\n",
        "        set[tuple[int, datetime]]: Множество пар (item_id, datetime).\n",
        "    \"\"\"\n",
        "    df_filtered = df.filter(col(\"user_id\") == user_id).select(\"item_id\", \"datetime\")\n",
        "\n",
        "    return set(df_filtered.rdd.map(lambda row: (row[\"item_id\"], row[\"datetime\"])).collect())\n",
        "\n",
        "\n",
        "def get_user_items_from_original(user_id: int, df: pyspark.sql.DataFrame, action: str) -> set[tuple[int, datetime]]:\n",
        "    \"\"\"\n",
        "    Получает множество пар (customer_id, timestamp) для пользователя из исходного датасета.\n",
        "\n",
        "    Args:\n",
        "        user_id (int): Идентификатор пользователя.\n",
        "        df (pyspark.sql.DataFrame): Исходный датасет с действиями юзеров (needed_beh_logs).\n",
        "        action (str): Название действия.\n",
        "\n",
        "    Returns:\n",
        "        set[tuple[int, datetime]]: Множество пар (customer_id, timestamp).\n",
        "    \"\"\"\n",
        "    df_filtered = (\n",
        "        df.filter(col(\"action\") == action)\n",
        "        .filter(col(\"customer_user_id\") == str(user_id))\n",
        "        .select(\"customer_id\", \"timestamp\")\n",
        "    )\n",
        "\n",
        "    df_filtered = df_filtered.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
        "\n",
        "    return set(df_filtered.rdd.map(lambda row: (int(row[\"customer_id\"]), row[\"timestamp\"].replace(microsecond=0))).collect())"
      ],
      "metadata": {
        "id": "Ul9fD1mKgrN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тесты для датасета с признаками товаров в формате SASRec"
      ],
      "metadata": {
        "id": "BimBi5hsjuDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Читаем файлы\n",
        "data_raw_items = spark.read.parquet(path_raw_items)\n",
        "data_processed_items = spark.read.parquet(path_processed_items)"
      ],
      "metadata": {
        "id": "W_ryJxGcQVcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.mark.parametrize(\"df\", [(data_processed_items)])\n",
        "def test_items_structure(df: pyspark.sql.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет структуру датасета с товарами.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): Предобработанный датасет с признаками товаров в формате SASRec.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    expected_columns = {\"id\", \"value\", \"feature\"}\n",
        "    actual_columns = set(df.columns)\n",
        "    assert actual_columns == expected_columns, f\"Ошибка: Ожидались колонки {expected_columns}, но получены {actual_columns}\"\n",
        "\n",
        "    schema = df.schema\n",
        "    assert schema[\"id\"].dataType.simpleString() == \"int\", \"Ошибка: 'id' должен быть целым числом\"\n",
        "    assert schema[\"value\"].dataType.simpleString() == \"string\", \"Ошибка: 'value' должен быть строкой\"\n",
        "    assert schema[\"feature\"].dataType.simpleString() == \"string\", \"Ошибка: 'feature' должен быть строкой\"\n",
        "\n",
        "    print(\"✅ В датасете правильные колонки и у них правильные типы данных\")\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"df\", [(data_processed_items)])\n",
        "def test_missing_values_items(df: pyspark.sql.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет наличие пропущенных значений в датасете.\n",
        "\n",
        "    Args:\n",
        "        df (pyspark.sql.DataFrame): Предобработанный датасет с признаками товаров в формате SASRec.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    missing_values = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "    missing_dict = missing_values.collect()[0].asDict()\n",
        "    total_missing = sum(missing_dict.values())\n",
        "    assert total_missing == 0, f\"Ошибка: Найдены пропущенные значения\\n{missing_dict}\"\n",
        "\n",
        "    print(\"✅ В датасете нет пропущенных значений\")\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"df_final, df_original\", [(data_processed_items, data_raw_items)])\n",
        "def test_number_items(df_final: pyspark.sql.DataFrame, df_original: pyspark.sql.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет соответствие числа товаров в обработанном и исходном датасетах.\n",
        "\n",
        "    Args:\n",
        "        df_final (pyspark.sql.DataFrame): Предобработанный датасет с признаками товаров в формате SASRec.\n",
        "        df_original (pyspark.sql.DataFrame): Исходный каталог (data_set_items).\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    n_items_original = df_original.select(\"customer_id\").distinct().count()\n",
        "    n_items_final = df_final.select(\"id\").distinct().count()\n",
        "    assert n_items_final == n_items_original, \"Ошибка: число товаров не совпадает с каталогом\"\n",
        "\n",
        "    print(\"✅ Число товаров в датасете совпадает с каталогом\")\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"df_final, df_original, metadata_original_names, facets_original_names, metadata_final_names, facets_final_names\", \\\n",
        " [(data_processed_items, data_raw_items,  metadata_original_names, facets_original_names, metadata_final_names, facets_final_names)])\n",
        "def test_items_features(\n",
        "    df_final: pyspark.sql.DataFrame,\n",
        "    df_original: pyspark.sql.DataFrame,\n",
        "    metadata_original_names: List[str],\n",
        "    facets_original_names: List[str],\n",
        "    metadata_final_names: List[str],\n",
        "    facets_final_names: List[str],\n",
        "    n_items: int = 3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Проверяет соответствие признаков товаров в обработанном и исходном датасетах.\n",
        "\n",
        "    Args:\n",
        "        df_final (pyspark.sql.DataFrame): Предобработанный датасет с признаками товаров в формате SASRec.\n",
        "        df_original (pyspark.sql.DataFrame): Исходный каталог (data_set_items).\n",
        "        metadata_original_names (List[str]): Список названий метаданных в исходном датасете.\n",
        "        facets_original_names (List[str]): Список названий фасетов в исходном датасете.\n",
        "        metadata_final_names (List[str]): Список названий метаданных в предобработанном датасете.\n",
        "        facets_final_names (List[str]): Список названий фасетов в предобработанном датасете.\n",
        "        n_items (int, optional): Количество случайных товаров для проверки. По умолчанию 10.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    items = df_final.select(\"id\").distinct().rdd.flatMap(lambda x: x).takeSample(False, n_items)\n",
        "    for item in tqdm(items):\n",
        "        check_item_features(item, df_final, df_original, metadata_original_names, facets_original_names, metadata_final_names, facets_final_names)\n",
        "\n",
        "    print(\"\\n✅ Признаки товаров в датасете совпадают с каталогом\")\n",
        "\n",
        "\n",
        "def check_item_features(\n",
        "    item_id: int,\n",
        "    df_final: pyspark.sql.DataFrame,\n",
        "    df_original: pyspark.sql.DataFrame,\n",
        "    metadata: List[str],\n",
        "    facets: List[str],\n",
        "    metadata_final_names: List[str],\n",
        "    facets_final_names: List[str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Проверяет корректность признаков конкретного товара.\n",
        "\n",
        "    Args:\n",
        "        item_id (int): Идентификатор товара.\n",
        "        df_final (pyspark.sql.DataFrame): Предобработанный датасет с признаками товаров в формате SASRec.\n",
        "        df_original (pyspark.sql.DataFrame): Исходный каталог (data_set_items).\n",
        "        metadata (List[str]): Список метаданных в исходном датасете.\n",
        "        facets (List[str]): Список фасетов в исходном датасете.\n",
        "        metadata_final_names (List[str]): Список названий метаданных в предобработанном датасете.\n",
        "        facets_final_names (List[str]): Список названий фасетов в предобработанном датасете.\n",
        "\n",
        "    Returns:\n",
        "        None: Функция выполняет проверки, но ничего не возвращает.\n",
        "    \"\"\"\n",
        "    features_original_names = metadata + facets\n",
        "    features_final_names = metadata_final_names + facets_final_names\n",
        "    features_names_changes = {features_original_names[i]: features_final_names[i] for i in range(len(features_original_names))}\n",
        "\n",
        "    item_features_final = item_features_from_final(item_id, df_final)\n",
        "    item_features_original = {features_names_changes[key]: value for key, value in item_features_from_original(item_id, df_original, metadata, facets).items()}\n",
        "\n",
        "    assert set(item_features_final).issubset(item_features_original), 'Ошибка: Финальный набор категорий содержит лишнюю категорию'\n",
        "    assert len(item_features_final) > 0, f'Ошибка: У товара {item_id} нет никаких признаков'\n",
        "\n",
        "\n",
        "def item_features_from_original(\n",
        "    item_id: int,\n",
        "    df: pyspark.sql.DataFrame,\n",
        "    metadata: List[str],\n",
        "    facets: Optional[List[str]] = None\n",
        ") -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Извлекает признаки товара из исходного датасета.\n",
        "\n",
        "    Args:\n",
        "        item_id (int): Идентификатор товара.\n",
        "        df (pyspark.sql.DataFrame): Исходный каталог (data_set_items).\n",
        "        metadata (List[str]): Список метаданных.\n",
        "        facets (List[str], optional): Список фасетов. По умолчанию None.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[str]]: Словарь с признаками товара.\n",
        "    \"\"\"\n",
        "    features_out = {}\n",
        "    df = df.filter(col('customer_id') == item_id).select('metadata_json')\n",
        "    metadata_in = json.loads(df.first()[0])\n",
        "\n",
        "    for metadata_type in metadata:\n",
        "        features_out[metadata_type] = metadata_in[metadata_type]\n",
        "\n",
        "    if facets:\n",
        "        facets_in = metadata_in.get('facets', {})\n",
        "        for facet in facets:\n",
        "            if facet in facets_in:\n",
        "                features_out[facet] = facets_in[facet]\n",
        "\n",
        "    return features_out\n",
        "\n",
        "\n",
        "def item_features_from_final(item_id: int, dataset: pyspark.sql.DataFrame) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Извлекает признаки товара из обработанного датасета.\n",
        "\n",
        "    Args:\n",
        "        item_id (int): Идентификатор товара.\n",
        "        dataset (pyspark.sql.DataFrame): Предобработанный датасет с признаками товаров в формате SASRec.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[str]]: Словарь, где ключ — это feature, а значения — список уникальных value.\n",
        "    \"\"\"\n",
        "    filtered_data = dataset.filter(dataset.id == item_id)\n",
        "    result = (\n",
        "        filtered_data\n",
        "        .groupBy(\"feature\")\n",
        "        .agg(collect_set(\"value\").alias(\"values\"))\n",
        "        .rdd\n",
        "        .map(lambda row: (row[\"feature\"], row[\"values\"]))\n",
        "        .collectAsMap()\n",
        "    )\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "ABTM4fX2Qk3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Своместные тесты на оба датасета в формате SASRec\n"
      ],
      "metadata": {
        "id": "w8NFXVMjTsyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.mark.parametrize(\"df_actions, df_items\", [(data_processed_actions, data_processed_items)])\n",
        "def test_items_with_actions_in_catalog(df_actions: pyspark.sql.DataFrame, df_items: pyspark.sql.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Проверяет, что почти все товары из действий пользователей присутствуют в каталоге товаров.\n",
        "\n",
        "    Args:\n",
        "        df_actions (DataFrame): Предобработанный датасет с действиями пользователей в формате SASRec.\n",
        "        df_items (DataFrame): Предобработанный датасет с признаками товаров в формате SASRec.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: Если в финальном датасете товаров слишком мало товаров\n",
        "                        или информация отсутствует более чем о 0.1% товаров.\n",
        "    \"\"\"\n",
        "    all_items_from_actions = set(df_actions.select(\"item_id\").distinct().rdd.map(lambda row: row[\"item_id\"]).collect())\n",
        "    all_items_from_catalog = set(df_items.select(\"id\").distinct().rdd.map(lambda row: row[\"id\"]).collect())\n",
        "\n",
        "    assert len(all_items_from_actions) <= len(all_items_from_catalog), 'Ошибка: В финальном датасете товаров слишком мало товаров'\n",
        "    assert len(all_items_from_actions - all_items_from_catalog) < 0.001 * len(all_items_from_actions), 'Ошибка: О многих товарах нет информации в финальном датасете товаров'\n",
        "\n",
        "    print(\"✅ В финальном датасете товаров есть 99.9% всех товаров, с которыми взаимодействовали юзеры\")"
      ],
      "metadata": {
        "id": "JZSlfW_WT0_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}