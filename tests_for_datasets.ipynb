{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6xxYyYw5gZlB"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_actions_sasrec(df_final, df_original, action):\n",
        "  test_actions_structure(df_final)\n",
        "  test_missing_values(df_final)\n",
        "  test_number_users(df_final, df_original, action)\n",
        "  test_users_actions(df_final, df_original, action)"
      ],
      "metadata": {
        "id": "sHI6JRuoLe3Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType, TimestampType, DoubleType\n",
        "\n",
        "def test_actions_structure(df):\n",
        "    # Проверка названий колонок\n",
        "    expected_columns = {\"user_id\", \"item_id\", \"datetime\", \"weight\"}\n",
        "    actual_columns = set(df.columns)\n",
        "    assert actual_columns == expected_columns, f\"Ошибка: Ожидались колонки {expected_columns}, но получены {actual_columns}\"\n",
        "\n",
        "    # Получаем схему DataFrame\n",
        "    schema = dict(df.dtypes)\n",
        "\n",
        "    # Проверка типов данных\n",
        "    assert schema[\"user_id\"] == \"int\", \"Ошибка: 'user_id' должен быть целым числом\"\n",
        "    assert schema[\"item_id\"] == \"int\", \"Ошибка: 'item_id' должен быть целым числом\"\n",
        "    assert schema[\"datetime\"] == \"timestamp\", \"Ошибка: 'datetime' должен быть datetime\"\n",
        "    assert schema[\"weight\"] == 'int', \"Ошибка: 'weight' должен быть числом\"\n",
        "\n",
        "    print(\"✅ В датасете правильные колонки и у них правильные типы данных\")\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "def test_missing_values(df):\n",
        "    # Считаем количество пропущенных значений в каждом столбце\n",
        "    missing_values = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "\n",
        "    # Преобразуем результат в словарь\n",
        "    missing_dict = missing_values.collect()[0].asDict()\n",
        "\n",
        "    # Проверяем, есть ли пропущенные значения\n",
        "    total_missing = sum(missing_dict.values())\n",
        "    assert total_missing == 0, f\"Ошибка: Найдены пропущенные значения\\n{missing_dict}\"\n",
        "\n",
        "    print(\"✅ В датасете нет пропущенных значений\")\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def test_number_users(df_final, df_original, action):\n",
        "    # Считаем количество уникальных пользователей в оригинальном DataFrame\n",
        "    n_users_original = (df_original\n",
        "                        .filter(col('action') == action)\n",
        "                        .select('customer_user_id')\n",
        "                        .na.drop()\n",
        "                        .distinct()\n",
        "                        .count())\n",
        "\n",
        "    # Считаем количество уникальных пользователей в финальном DataFrame\n",
        "    n_users_final = df_final.select('user_id').distinct().count()\n",
        "\n",
        "    # Проверки\n",
        "    assert n_users_final <= n_users_original, 'Ошибка: слишком большое число юзеров'\n",
        "    assert n_users_final > 0.95 * n_users_original, 'Ошибка: пропало больше 5% нужных юзеров'\n",
        "\n",
        "    print(\"✅ В датасете правильное число юзеров, которые сделали действие\")\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import random\n",
        "\n",
        "def test_users_actions(df_final, df_original, action, n_users=10):\n",
        "    # Выбираем случайных пользователей\n",
        "    users = df_final.select(\"user_id\").distinct().rdd.flatMap(lambda x: x).takeSample(False, n_users)\n",
        "\n",
        "    for user in users:\n",
        "        test_user_actions(user, df_final, df_original, action)\n",
        "\n",
        "    print(\"✅ Для всех юзеров правильно указаны товары и время действий\")\n",
        "\n",
        "def test_user_actions(user_id, df_final, df_original, action):\n",
        "    assert get_user_items_from_final(user_id, df_final) == get_user_items_from_original(user_id, df_original, action), \\\n",
        "        f\"Ошибка: неправильно указаны набор товаров и время действий для юзера {user_id}\"\n",
        "\n",
        "def get_user_items_from_final(user_id, df):\n",
        "    df_filtered = df.filter(col(\"user_id\") == user_id).select(\"item_id\", \"datetime\")\n",
        "\n",
        "    # Собираем в сет кортежей (item_id, datetime)\n",
        "    result = set(df_filtered.rdd.map(lambda row: (row[\"item_id\"], row[\"datetime\"])).collect())\n",
        "\n",
        "    return result\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "from pyspark.sql.types import TimestampType\n",
        "from datetime import datetime\n",
        "\n",
        "def get_user_items_from_original(user_id, df, action):\n",
        "    df_filtered = (df.filter(col(\"action\") == action)\n",
        "                     .filter(col(\"customer_user_id\") == str(user_id))\n",
        "                     .select(\"customer_id\", \"timestamp\"))\n",
        "\n",
        "    # Преобразуем в тип Timestamp\n",
        "    df_filtered = df_filtered.withColumn(\n",
        "        \"timestamp\", to_timestamp(col(\"timestamp\"))\n",
        "    )\n",
        "\n",
        "    # Преобразуем в set кортежей (customer_id, timestamp), убираем микросекунды\n",
        "    result = set(df_filtered.rdd.map(lambda row: (int(row[\"customer_id\"]), row[\"timestamp\"].replace(microsecond=0))).collect())\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "Ul9fD1mKgrN4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_items_sasrec(df_final, df_original, metadata_original_names, facets_original_names, features_final_names):\n",
        "  test_items_structure(df_final)\n",
        "  test_missing_values(df_final)\n",
        "  test_number_items(df_final, df_original)\n",
        "  test_items_features(df_final, df_original, metadata_original_names, facets_original_names, features_final_names)"
      ],
      "metadata": {
        "id": "VXU5qaM6YUIN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from tqdm import tqdm\n",
        "\n",
        "def test_items_structure(df):\n",
        "    # Проверка названий колонок\n",
        "    expected_columns = {\"id\", \"value\", \"feature\"}\n",
        "    actual_columns = set(df.columns)\n",
        "    assert actual_columns == expected_columns, f\"Ошибка: Ожидались колонки {expected_columns}, но получены {actual_columns}\"\n",
        "\n",
        "    # Проверка типов данных\n",
        "    schema = df.schema\n",
        "    assert schema[\"id\"].dataType.simpleString() == \"int\", \"Ошибка: 'id' должен быть целым числом\"\n",
        "    assert schema[\"value\"].dataType.simpleString() == \"string\", \"Ошибка: 'value' должен быть строкой\"\n",
        "    assert schema[\"feature\"].dataType.simpleString() == \"string\", \"Ошибка: 'feature' должен быть строкой\"\n",
        "\n",
        "    print(\"✅ В датасете правильные колонки и у них правильные типы данных\")\n",
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "def test_missing_values(df):\n",
        "    # Считаем количество пропущенных значений в каждом столбце\n",
        "    missing_values = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
        "\n",
        "    # Преобразуем результат в словарь\n",
        "    missing_dict = missing_values.collect()[0].asDict()\n",
        "\n",
        "    # Проверяем, есть ли пропущенные значения\n",
        "    total_missing = sum(missing_dict.values())\n",
        "    assert total_missing == 0, f\"Ошибка: Найдены пропущенные значения\\n{missing_dict}\"\n",
        "\n",
        "    print(\"✅ В датасете нет пропущенных значений\")\n",
        "\n",
        "\n",
        "\n",
        "def test_number_items(df_final, df_original):\n",
        "    n_items_original = df_original.select(\"customer_id\").distinct().count()\n",
        "    n_items_final = df_final.select(\"id\").dropna().distinct().count()\n",
        "\n",
        "    assert n_items_final == n_items_original, \"Ошибка: число товаров не совпадает с каталогом\"\n",
        "\n",
        "    print(\"✅ Тест на множество товаров пройден\")\n",
        "\n",
        "\n",
        "def test_items_features(df_final, df_original, metadata_original_names, facets_original_names, features_final_names, n_items=10):\n",
        "    # Выбираем случайных пользователей\n",
        "    items = df_final.select(\"id\").distinct().rdd.flatMap(lambda x: x).takeSample(False, n_items)\n",
        "\n",
        "    for item in tqdm(items):\n",
        "        test_item_features(item, df_final, df_original, metadata_original_names, facets_original_names, features_final_names)\n",
        "\n",
        "    print(\"\\n✅ Признаки товаров в датасете совпадают с каталогом\")\n",
        "\n",
        "\n",
        "def test_item_features(item_id, df_final, df_original, metadata, facets, features_final_names):\n",
        "  features_original_names = metadata + facets\n",
        "  features_names_changes = {features_original_names[i] : features_final_names[i] for i in range(len(features_original_names))}\n",
        "\n",
        "  item_features_final = item_features_from_final(item_id, df_final)\n",
        "  item_features_original = {features_names_changes[key] : value for key, value in item_features_from_original(item_id, df_original, metadata, facets).items()}\n",
        "  assert set(item_features_final).issubset(item_features_original), 'Ошибка: Финальный набор категорий содержит лишнюю категорию'\n",
        "  assert len(item_features_final) > 0, f'Ошибка: У товара {item_id} нет никаких признаков'\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "def item_features_from_original(item_id, df, metadata, facets=None):\n",
        "  features_out = {}\n",
        "  df = df.filter(col('customer_id')==item_id).select('metadata_json')\n",
        "  metadata_in = json.loads(df.first()[0])\n",
        "  for metadata_type in metadata:\n",
        "    features_out[metadata_type] = metadata_in[metadata_type]\n",
        "\n",
        "  if facets:\n",
        "    facets_in = metadata_in['facets']\n",
        "    for facet in facets:\n",
        "      if facet in facets_in.keys():\n",
        "        features_out[facet] = facets_in[facet]\n",
        "  return features_out\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import collect_set\n",
        "def item_features_from_final(item_id, dataset):\n",
        "    \"\"\"\n",
        "    Функция принимает item_id и PySpark DataFrame, а возвращает словарь,\n",
        "    где ключи — это уникальные значения из столбца feature,\n",
        "    а значения — списки уникальных значений из столбца value для данного item_id.\n",
        "    \"\"\"\n",
        "    # Фильтруем датасет по item_id\n",
        "    filtered_data = dataset.filter(dataset.id == item_id)\n",
        "\n",
        "    # Группируем по feature и собираем уникальные значения value\n",
        "    result = (\n",
        "        filtered_data\n",
        "        .groupBy(\"feature\")\n",
        "        .agg(collect_set(\"value\").alias(\"values\"))\n",
        "        .rdd\n",
        "        .map(lambda row: (row[\"feature\"], row[\"values\"]))\n",
        "        .collectAsMap()\n",
        "    )\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "dPyc-sDG4tb6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def common_tests_sasrec(df_actions, df_items):\n",
        "  test_conversed_items_in_catalog(df_actions, df_items)"
      ],
      "metadata": {
        "id": "jsr7G9dmxTAW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_conversed_items_in_catalog(df_actions, df_items):\n",
        "  all_items_from_actions = set(df_actions.select(\"item_id\").distinct().rdd.map(lambda row: row[\"item_id\"]).collect())\n",
        "  all_items_from_catalog = set(df_items.select(\"id\").distinct().rdd.map(lambda row: row[\"id\"]).collect())\n",
        "\n",
        "  print(len(all_items_from_actions), len(all_items_from_catalog))\n",
        "  assert len(all_items_from_actions) <= len(all_items_from_catalog), 'Ошибка: В финальном датасете товаров слишком мало товаров'\n",
        "  assert len(all_items_from_actions - all_items_from_catalog) < 0.001*len(all_items_from_actions), 'Ошибка: О многих товарах нет информации в финальном датасете товаров'\n",
        "\n",
        "  print(\"✅ В финальном датасете товаров есть 99.9% всех необходимых товаров\")"
      ],
      "metadata": {
        "id": "20aAoYCHYM_t"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}