# -*- coding: utf-8 -*-
"""test_petco_direct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmQejt7mVZVIbzYNSm9KriHO8yvnRy04
"""

# Import libraries
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
import matplotlib.pyplot as plt
import pandas as pd
from pyspark.sql.functions import col
from typing import List, Dict, Tuple, Set, Optional, Callable
from pyspark.sql.functions import to_timestamp
import pyspark
import pytest

# PARAMETERS
WORKING_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/diploma/'
path_raw_actions = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/raw/actions'
path_raw_items = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/raw/items'
path_expected_actions = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/sasrec_format_expected/actions'
path_expected_items = WORKING_DIRECTORY+'tests/tests_petco/toy_examples/toy_example_1/sasrec_format_expected/items'
action = 'conversion'
metadata = ['url', 'image_url', 'itemname', 'group_ids']
facets = ['How to get it', 'Primary Brand', 'Pet Type', 'Primary Pet Type']
features = ['group_ids_intersect', 'How_to_get_it', 'Primary_Brand', 'Primary_Pet_Type']
features_final_names = ['Category', 'Delivery', 'Brand', 'Pet']

from pyspark.sql.functions import col

def are_identical(df1: pyspark.sql.DataFrame, df2:pyspark.sql.DataFrame):
    # Проверка, что количество строк одинаковое
    assert df1.count() == df2.count()

    # Проверка, что структура одинаковая (количество столбцов, имена столбцов и типы)
    assert df1.columns == df2.columns and df1.dtypes == df2.dtypes

    # Сравниваем содержимое строк
    assert df1.exceptAll(df2).count() == 0 and df2.exceptAll(df1).count() == 0

def start_script():
  # Создаём SparkSession
  spark = SparkSession.builder \
      .appName("PetCo_1") \
      .getOrCreate()

  # Добавляем файлы  в Spark-контекст
  from pyspark import SparkFiles
  spark.sparkContext.addFile("/content/drive/MyDrive/Colab Notebooks/diploma/scripts/process_data.py")
  import process_data

  return spark, process_data

"""# Тест для датасета с действиями юзеров в формате SASRec"""

# Создаем SparkSession и добавляем файлы  в Spark-контекст
spark, process_data = start_script()

@pytest.mark.parametrize("path_raw_actions, path_expected_actions, action", [(path_raw_actions, path_expected_actions, action)])
def test_actions_sasrec(path_raw_actions : str, path_expected_actions: str, action: str):
  data_raw_actions = spark.read.parquet(path_raw_actions)
  data_actions_cleaned = process_data.clean_data_actions(data_raw_actions, action)
  data_actions_processed = process_data.actions_to_sasrec_format(data_actions_cleaned)

  data_actions_expected = spark.read.parquet(path_expected_actions)

  are_identical(data_actions_processed, data_actions_expected)

"""# Тест для датасета с признакми товаров в формате SASRec"""

@pytest.mark.parametrize("path_raw_items, path_expected_items, metadata, facets, features, features_final_names", \
 [(path_raw_items, path_expected_items, metadata, facets, features, features_final_names)])
def test_items_sasrec(path_raw_items : str,
                      path_expected_items: str,
                      metadata: List[str],
                      facets: List[str],
                      features: List[str],
                      features_final_names: List[str],):

  spark, process_data = start_script()
  data_raw_items = spark.read.parquet(path_raw_items)
  data_items_cleaned = process_data.clean_data_items(data_raw_items, metadata = metadata, facets = facets)
  data_items_processed = process_data.items_to_sasrec_format(data_items_cleaned, features, features_final_names)

  data_items_expected = spark.read.parquet(path_expected_items)
  are_identical(data_items_processed, data_items_expected)

